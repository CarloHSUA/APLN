{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\berna\\OneDrive\\Escritorio\\master_ia\\APLN\\APLN\\.env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\berna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from gensim.corpora import Dictionary  # Mapeo entre palabras e ids\n",
    "from gensim.models import LdaModel  # Cargar modeo LDA\n",
    "from gensim.parsing.preprocessing import STOPWORDS  # Lista de palabras de parada\n",
    "from gensim.test.utils import datapath  # Utilidad para guardar y cargar modelos\n",
    "from gensim.utils import (\n",
    "    simple_preprocess,\n",
    ")  # Convertir un documento en una lista de tokens\n",
    "import nltk  # Natural Language Toolkit\n",
    "from nltk.stem import WordNetLemmatizer  # Lematizado\n",
    "import os\n",
    "\n",
    "# Descargamos información de WordNet para el lematizado\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glnmario/news-qa-summarization\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = list()\n",
    "for article in dataset:\n",
    "    stories.append(article[\"story\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) > 3:\n",
    "            result.append(WordNetLemmatizer().lemmatize(token))\n",
    "    return result\n",
    "processed_docs = [preprocess(story) for story in stories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "able action actor afghan afghan afghan afghan afghan afghan afghan afghan afghan afghan afghan afghan afghan afghanistan afghanistan afghanistan afghanistan afghanistan afghanistan afghanistan afghanistan afghanistan afghanistan alleyway approach army army army army army aroma asylum asylum authorized baking banter battalion battered bazaar beam beria beria best better british british british british british british british british british british british build business center changing clearly closest clothes colthup colthup come comfortable complex complex compound conduct cost count countryside course creating creating cross cultural cultural culture culture culture david difficult directly distinctive duty easily easy elder elder elder england england english entering environment exercise explains explains farmhouse farmhouse fazel feature fight flatbread fled force force force freshly ghurkha give going grape green grilled ground ground group hand handle hawk heart helmand helmand high high hindu home hour identifiable instead insurgent interact interact interpreter ireland issue join kalay kalay kalay kalay kalay kalay kalay know kush ladscape laugh legged life like little looking lurk mark market market market melon mentoring meter million mimic mind mirror mock mock motorcycle mountain narrow need need neighborhood nepalese norfolk northern operate opium partake pashtun patrol patrol patrol patrol people photo physical pipe play play played police political position previously pride production province province quick real reality recruiting regiment religion religious relish resembled respect responsible right role role role say say say search security security security seeker seeker sensitivity served serving serving short sight silently sindh sindh sindh sindh sindh sindh sindh sitting smell smoking soldier soldier soldier soldier soldier soldier soldier soldier soldier soldier soldier soldier staffed state stretch stronghold surprise suspicious tactic taliban taliban taliban talk tasked terrain thing thing think tidy today tour train train trained training training training training training training training training troop troop troop troop troop troop ultimately ultimately unable understand unit urban useful vendor vendor village village village village village village village village waft walking wall want watch water weapon weapon welcome went went western woman wrong yorkshire\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(processed_docs)\n",
    "corpus = [dictionary.doc2bow(story) for story in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(\n",
    "        corpus=corpus,  # El conjunto de datos\n",
    "        id2word=dictionary,  # Diccionario que mapea cada palabra a un identificador único\n",
    "        num_topics=10,  # Número de temas que queremos identificar\n",
    "        random_state=666,  # Establecemos una semilla para reproducibilidad\n",
    "        update_every=1,  # Cada cuantos documentos se actualizan los parámetros del modelo\n",
    "        passes=15,  # Núero de pases sobre el corpus completo. Más pases implica más precisión, pero más tiempo de entrenamiento\n",
    "        alpha=\"auto\",  # Cantidad de temas que habrá en los documentos (si normalmente habrán pocos o muchos)\n",
    "        per_word_topics=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.032*\"said\" + 0.015*\"military\" + 0.014*\"official\" + 0.010*\"attack\" + 0.010*\"force\" + 0.009*\"afghanistan\"')\n",
      "(1, '0.020*\"child\" + 0.018*\"said\" + 0.017*\"school\" + 0.012*\"health\" + 0.011*\"student\" + 0.010*\"woman\"')\n",
      "(2, '0.032*\"said\" + 0.014*\"court\" + 0.009*\"year\" + 0.009*\"case\" + 0.007*\"told\" + 0.007*\"family\"')\n",
      "(3, '0.019*\"said\" + 0.008*\"people\" + 0.008*\"church\" + 0.007*\"group\" + 0.007*\"government\" + 0.007*\"country\"')\n",
      "(4, '0.009*\"year\" + 0.009*\"like\" + 0.007*\"time\" + 0.007*\"people\" + 0.006*\"say\" + 0.005*\"world\"')\n",
      "(5, '0.012*\"team\" + 0.010*\"game\" + 0.010*\"world\" + 0.010*\"year\" + 0.009*\"player\" + 0.008*\"second\"')\n",
      "(6, '0.048*\"said\" + 0.013*\"police\" + 0.008*\"people\" + 0.006*\"home\" + 0.006*\"told\" + 0.006*\"city\"')\n",
      "(7, '0.018*\"said\" + 0.013*\"iran\" + 0.012*\"government\" + 0.011*\"state\" + 0.010*\"election\" + 0.010*\"country\"')\n",
      "(8, '0.031*\"said\" + 0.020*\"flight\" + 0.016*\"plane\" + 0.012*\"airport\" + 0.011*\"pilot\" + 0.010*\"airline\"')\n",
      "(9, '0.021*\"said\" + 0.016*\"obama\" + 0.014*\"president\" + 0.009*\"state\" + 0.009*\"house\" + 0.007*\"american\"')\n"
     ]
    }
   ],
   "source": [
    "topics = lda_model.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignamos los temas a cada documento en el corpus\n",
    "topics_dict = dict()\n",
    "for document_num, doc in enumerate(corpus):\n",
    "  # Obtenemos las distribución de temas para el documento\n",
    "  doc_topics, word_topics, phi_values = lda_model.get_document_topics(doc, per_word_topics=True)\n",
    "  \n",
    "  # Ordenamos por el segundo elemento de la tupla (probabilidad del tema) y obtenemos el mayor\n",
    "  dominant_topic = sorted(doc_topics, key=lambda x: x[1], reverse=True)[0]\n",
    "  topic_num, prop_topic = dominant_topic\n",
    "\n",
    "  # Crear una lista vacía para almacenar las palabras del documento\n",
    "  document_words = []\n",
    "\n",
    "  # Recorrer cada par (id de palabra, frecuencia) en la representación BoW\n",
    "  for word_id, freq in doc:\n",
    "    # Obtener la palabra correspondiente al id del diccionario\n",
    "    word = dictionary[word_id]\n",
    "    \n",
    "    # Agregar la palabra a la lista de palabras del documento, repetida según su frecuencia\n",
    "    document_words.extend([word] * freq)\n",
    "  # Unir las palabras en un solo string, separadas por espacios\n",
    "  document = ' '.join(document_words)\n",
    "  # Guardado de texto por tema\n",
    "  if topic_num in topics_dict:\n",
    "    topics_dict[topic_num] += document\n",
    "  else:\n",
    "    topics_dict[topic_num] = document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "os.makedirs(\"wordclouds\", exist_ok=True)\n",
    "\n",
    "for topic_num, words in topics_dict.items():\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = STOPWORDS,\n",
    "                collocations=False, \n",
    "                min_font_size = 10).generate(words)\n",
    "    wordcloud.to_file(f\"wordclouds/topic_{topic_num}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
